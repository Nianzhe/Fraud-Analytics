{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.record < 833508]\n",
    "df2 = df[df.record >= 833508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.set_index('record')\n",
    "df2 = df2.set_index('record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop('fraud_label', axis = 1)\n",
    "Y = df1['fraud_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oot = df2.drop('fraud_label', axis = 1)\n",
    "y_oot = df2['fraud_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address-homephone-zip5_count180_date</th>\n",
       "      <th>address-homephone-zip5_count7_date</th>\n",
       "      <th>address-homephone-zip5_pastday</th>\n",
       "      <th>address-homephone_count180_date</th>\n",
       "      <th>address-homephone_pastday</th>\n",
       "      <th>address-zip5_0_count180_count_ratio</th>\n",
       "      <th>address-zip5_0_count30_count_ratio</th>\n",
       "      <th>address-zip5_0_count7_count_ratio</th>\n",
       "      <th>address-zip5_count30_date</th>\n",
       "      <th>address-zip5_pastday</th>\n",
       "      <th>dob-name_count30_date</th>\n",
       "      <th>dob-name_pastday</th>\n",
       "      <th>homephone-zip5_count30_date</th>\n",
       "      <th>ssn-dob_pastday</th>\n",
       "      <th>ssn-name_count180_date</th>\n",
       "      <th>ssn-name_count7_date</th>\n",
       "      <th>ssn-name_pastday</th>\n",
       "      <th>ssn_count180_date</th>\n",
       "      <th>ssn_unique_address-zip5</th>\n",
       "      <th>fraud_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>record</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        address-homephone-zip5_count180_date  \\\n",
       "record                                         \n",
       "1                                          1   \n",
       "2                                          1   \n",
       "3                                          1   \n",
       "4                                          1   \n",
       "5                                          1   \n",
       "\n",
       "        address-homephone-zip5_count7_date  address-homephone-zip5_pastday  \\\n",
       "record                                                                       \n",
       "1                                        1                           365.0   \n",
       "2                                        1                           365.0   \n",
       "3                                        1                           365.0   \n",
       "4                                        1                           365.0   \n",
       "5                                        1                           365.0   \n",
       "\n",
       "        address-homephone_count180_date  address-homephone_pastday  \\\n",
       "record                                                               \n",
       "1                                     1                      365.0   \n",
       "2                                     1                      365.0   \n",
       "3                                     1                      365.0   \n",
       "4                                     1                      365.0   \n",
       "5                                     1                      365.0   \n",
       "\n",
       "        address-zip5_0_count180_count_ratio  \\\n",
       "record                                        \n",
       "1                                  0.005556   \n",
       "2                                  0.005556   \n",
       "3                                  0.005556   \n",
       "4                                  0.005556   \n",
       "5                                  0.005556   \n",
       "\n",
       "        address-zip5_0_count30_count_ratio  address-zip5_0_count7_count_ratio  \\\n",
       "record                                                                          \n",
       "1                                 0.033333                           0.142857   \n",
       "2                                 0.033333                           0.142857   \n",
       "3                                 0.033333                           0.142857   \n",
       "4                                 0.033333                           0.142857   \n",
       "5                                 0.033333                           0.142857   \n",
       "\n",
       "        address-zip5_count30_date  address-zip5_pastday  \\\n",
       "record                                                    \n",
       "1                               1                 365.0   \n",
       "2                               1                 365.0   \n",
       "3                               1                 365.0   \n",
       "4                               1                 365.0   \n",
       "5                               1                 365.0   \n",
       "\n",
       "        dob-name_count30_date  dob-name_pastday  homephone-zip5_count30_date  \\\n",
       "record                                                                         \n",
       "1                           1             365.0                            1   \n",
       "2                           1             365.0                            1   \n",
       "3                           1             365.0                            1   \n",
       "4                           1             365.0                            1   \n",
       "5                           1             365.0                            1   \n",
       "\n",
       "        ssn-dob_pastday  ssn-name_count180_date  ssn-name_count7_date  \\\n",
       "record                                                                  \n",
       "1                 365.0                       1                     1   \n",
       "2                 365.0                       1                     1   \n",
       "3                 365.0                       1                     1   \n",
       "4                 365.0                       1                     1   \n",
       "5                 365.0                       1                     1   \n",
       "\n",
       "        ssn-name_pastday  ssn_count180_date  ssn_unique_address-zip5  \\\n",
       "record                                                                 \n",
       "1                  365.0                  1                        1   \n",
       "2                  365.0                  1                        1   \n",
       "3                  365.0                  1                        1   \n",
       "4                  365.0                  1                        1   \n",
       "5                  365.0                  1                        1   \n",
       "\n",
       "        fraud_label  \n",
       "record               \n",
       "1                 0  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 0  \n",
       "5                 0  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=scaler.fit_transform(x_train.to_numpy())\n",
    "x_train=pd.DataFrame(x_train,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=scaler.fit_transform(x_test.to_numpy())\n",
    "x_test=pd.DataFrame(x_test,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oot=scaler.fit_transform(x_oot.to_numpy())\n",
    "x_oot=pd.DataFrame(x_oot,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166493"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_oot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=19, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 13s 20us/step - loss: 0.6581 - acc: 0.6580 - auc: 0.6637 - val_loss: 0.5171 - val_acc: 0.9891 - val_auc: 0.6104\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.4007 - acc: 0.9898 - auc: 0.6213 - val_loss: 0.2523 - val_acc: 0.9902 - val_auc: 0.6377\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 14s 20us/step - loss: 0.1547 - acc: 0.9904 - auc: 0.6532 - val_loss: 0.0913 - val_acc: 0.9904 - val_auc: 0.6688\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 14s 21us/step - loss: 0.0701 - acc: 0.9906 - auc: 0.6824 - val_loss: 0.0581 - val_acc: 0.9906 - val_auc: 0.6944\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.0528 - acc: 0.9910 - auc: 0.7038 - val_loss: 0.0505 - val_acc: 0.9910 - val_auc: 0.7116\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0485 - acc: 0.9912 - auc: 0.7177 - val_loss: 0.0483 - val_acc: 0.9910 - val_auc: 0.7227\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0471 - acc: 0.9912 - auc: 0.7270 - val_loss: 0.0475 - val_acc: 0.9910 - val_auc: 0.7302\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7328 - val_loss: 0.0472 - val_acc: 0.9910 - val_auc: 0.7355\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0463 - acc: 0.9913 - auc: 0.7377 - val_loss: 0.0470 - val_acc: 0.9910 - val_auc: 0.7395\n",
      "Epoch 10/10\n",
      "147200/666805 [=====>........................] - ETA: 7s - loss: 0.0448 - acc: 0.9915 - auc: 0.7402"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10,validation_data=(x_test,y_test), batch_size=6400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDR(x,y,model):\n",
    "    pred = model.predict(x)\n",
    "    y_df = pd.DataFrame(y)\n",
    "    y_df['pred'] = pred\n",
    "    top = int(len(y_df) * 0.03)\n",
    "    numbads = sum(y_df['fraud_label'] == 1)\n",
    "    fdr = y_df.sort_values(by = 'pred', ascending = False).head(top).fraud_label.sum()/numbads\n",
    "    return fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR(x_oot,y_oot,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns = ['first_layer','second_layer', 'epoch','batchsize','train_fdr','test_fdr','oot_fdr' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(first_layer, second_layer, epoch, batchsize):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_layer, input_dim=19, activation='relu'))\n",
    "    model.add(Dense(second_layer, activation='relu'))\n",
    "    model.add(Dense(output_dim = 1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])\n",
    "    model.fit(x_train, y_train, epochs=epoch,validation_data=(x_test,y_test),batch_size=batchsize )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.4374 - acc: 0.8907 - auc: 0.4359 - val_loss: 0.2094 - val_acc: 0.9902 - val_auc: 0.5953\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0968 - acc: 0.9905 - auc: 0.6537 - val_loss: 0.0508 - val_acc: 0.9907 - val_auc: 0.6918\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0473 - acc: 0.9908 - auc: 0.7108 - val_loss: 0.0472 - val_acc: 0.9908 - val_auc: 0.7243\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9910 - auc: 0.7316 - val_loss: 0.0469 - val_acc: 0.9909 - val_auc: 0.7378\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9911 - auc: 0.7416 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7449\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0459 - acc: 0.9911 - auc: 0.7472 - val_loss: 0.0466 - val_acc: 0.9910 - val_auc: 0.7493\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0457 - acc: 0.9911 - auc: 0.7509 - val_loss: 0.0465 - val_acc: 0.9910 - val_auc: 0.7522\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0456 - acc: 0.9912 - auc: 0.7534 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7544\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0454 - acc: 0.9912 - auc: 0.7552 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7560\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7566 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7573\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 13s 19us/step - loss: 0.2757 - acc: 0.9709 - auc: 0.4828 - val_loss: 0.0547 - val_acc: 0.9903 - val_auc: 0.6454\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0489 - acc: 0.9904 - auc: 0.6975 - val_loss: 0.0483 - val_acc: 0.9905 - val_auc: 0.7216\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0473 - acc: 0.9908 - auc: 0.7320 - val_loss: 0.0478 - val_acc: 0.9908 - val_auc: 0.7391\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0468 - acc: 0.9910 - auc: 0.7430 - val_loss: 0.0474 - val_acc: 0.9910 - val_auc: 0.7463\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0464 - acc: 0.9912 - auc: 0.7481 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7502\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0461 - acc: 0.9912 - auc: 0.7513 - val_loss: 0.0469 - val_acc: 0.9910 - val_auc: 0.7527\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7536 - val_loss: 0.0467 - val_acc: 0.9910 - val_auc: 0.7545\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9912 - auc: 0.7551 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7558\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7562 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7568\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7572 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7576\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.4153 - acc: 0.9699 - auc: 0.3306 - val_loss: 0.2030 - val_acc: 0.9899 - val_auc: 0.5149\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.1106 - acc: 0.9903 - auc: 0.6040 - val_loss: 0.0586 - val_acc: 0.9906 - val_auc: 0.6636\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0493 - acc: 0.9908 - auc: 0.6908 - val_loss: 0.0478 - val_acc: 0.9909 - val_auc: 0.7090\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0468 - acc: 0.9911 - auc: 0.7191 - val_loss: 0.0473 - val_acc: 0.9910 - val_auc: 0.7272\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7323 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7366\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9913 - auc: 0.7401 - val_loss: 0.0468 - val_acc: 0.9912 - val_auc: 0.7423\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7439 - val_loss: 0.0467 - val_acc: 0.9912 - val_auc: 0.7461\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7474 - val_loss: 0.0466 - val_acc: 0.9912 - val_auc: 0.7489\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7499 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7510\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9914 - auc: 0.7518 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7527\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7533 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7540\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7545 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7551\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7555 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7560\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7564 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7569\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7572 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7576\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7579 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7582\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7586 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7588\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7590 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7593\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7594 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7597\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7600 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7602\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.1690 - acc: 0.9881 - auc: 0.6024 - val_loss: 0.0515 - val_acc: 0.9905 - val_auc: 0.7122\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0481 - acc: 0.9908 - auc: 0.7321 - val_loss: 0.0481 - val_acc: 0.9909 - val_auc: 0.7431\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0469 - acc: 0.9912 - auc: 0.7474 - val_loss: 0.0475 - val_acc: 0.9911 - val_auc: 0.7506\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7528 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7541\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9913 - auc: 0.7548 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7560\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7565 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7574\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7579 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7585\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9914 - auc: 0.7588 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7593\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7596 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7600\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7603 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7606\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7608 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7611\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7614 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7616\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7616 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7619\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7621 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7623\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7624 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7626\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7627 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7628\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7630 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7631\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7632 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7633\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0449 - acc: 0.9915 - auc: 0.7634 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7635\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7636 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7637\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.2920 - acc: 0.9879 - auc: 0.5948 - val_loss: 0.1233 - val_acc: 0.9901 - val_auc: 0.6398\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0777 - acc: 0.9903 - auc: 0.6686 - val_loss: 0.0545 - val_acc: 0.9903 - val_auc: 0.6951\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0503 - acc: 0.9904 - auc: 0.7120 - val_loss: 0.0495 - val_acc: 0.9903 - val_auc: 0.7238\n",
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0481 - acc: 0.9905 - auc: 0.7308 - val_loss: 0.0484 - val_acc: 0.9903 - val_auc: 0.7365\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0474 - acc: 0.9907 - auc: 0.7401 - val_loss: 0.0480 - val_acc: 0.9904 - val_auc: 0.7432\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0470 - acc: 0.9909 - auc: 0.7452 - val_loss: 0.0476 - val_acc: 0.9908 - val_auc: 0.7473\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0467 - acc: 0.9910 - auc: 0.7486 - val_loss: 0.0474 - val_acc: 0.9909 - val_auc: 0.7501\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9911 - auc: 0.7509 - val_loss: 0.0473 - val_acc: 0.9908 - val_auc: 0.7521\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9911 - auc: 0.7529 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7536\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0462 - acc: 0.9912 - auc: 0.7542 - val_loss: 0.0470 - val_acc: 0.9909 - val_auc: 0.7548\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0460 - acc: 0.9912 - auc: 0.7553 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7558\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7562 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7566\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7569 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7574\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7575 - val_loss: 0.0467 - val_acc: 0.9910 - val_auc: 0.7580\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7582 - val_loss: 0.0466 - val_acc: 0.9910 - val_auc: 0.7585\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7587 - val_loss: 0.0466 - val_acc: 0.9910 - val_auc: 0.7590\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7592 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7594\n",
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7596 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7598\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7599 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7602\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7604 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7605\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7606 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7608\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7609 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7611\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7612 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7613\n",
      "Epoch 24/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7614 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7615\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7616 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7618\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7619 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7620\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7620 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7622\n",
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7622 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7623\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7624 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7625\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7625 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7627\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.3149 - acc: 0.9760 - auc: 0.5019 - val_loss: 0.0636 - val_acc: 0.9904 - val_auc: 0.6692\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0497 - acc: 0.9908 - auc: 0.7071 - val_loss: 0.0475 - val_acc: 0.9908 - val_auc: 0.7279\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0466 - acc: 0.9911 - auc: 0.7364 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0462 - acc: 0.9911 - auc: 0.7459 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7492\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9912 - auc: 0.7513 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7529\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0458 - acc: 0.9912 - auc: 0.7538 - val_loss: 0.0467 - val_acc: 0.9910 - val_auc: 0.7554\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7560 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7571\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7578 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7585\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7589 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7595\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7600 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7604\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7607 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7610\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7614 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7616\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 13s 20us/step - loss: 0.0451 - acc: 0.9913 - auc: 0.7618 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7621\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7623 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7625\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7627 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7629\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7630 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7632\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7633 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7635\n",
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7635 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7638\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7638 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7640\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7640 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7642\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7643 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7644\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7645 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7645\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7646 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7647\n",
      "Epoch 24/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7647 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7648\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7648 - val_loss: 0.0459 - val_acc: 0.9911 - val_auc: 0.7649\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7650 - val_loss: 0.0459 - val_acc: 0.9912 - val_auc: 0.7651\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7651 - val_loss: 0.0459 - val_acc: 0.9912 - val_auc: 0.7652\n",
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7652 - val_loss: 0.0459 - val_acc: 0.9912 - val_auc: 0.7653\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7654 - val_loss: 0.0459 - val_acc: 0.9912 - val_auc: 0.7653\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7653 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7654\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.4834 - acc: 0.9660 - auc: 0.3756 - val_loss: 0.3100 - val_acc: 0.9898 - val_auc: 0.5036\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.1917 - acc: 0.9901 - auc: 0.5725 - val_loss: 0.1025 - val_acc: 0.9905 - val_auc: 0.6235\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0673 - acc: 0.9907 - auc: 0.6558 - val_loss: 0.0527 - val_acc: 0.9907 - val_auc: 0.6804\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0491 - acc: 0.9908 - auc: 0.6955 - val_loss: 0.0487 - val_acc: 0.9908 - val_auc: 0.7072\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0473 - acc: 0.9909 - auc: 0.7150 - val_loss: 0.0480 - val_acc: 0.9911 - val_auc: 0.7217\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0469 - acc: 0.9912 - auc: 0.7264 - val_loss: 0.0477 - val_acc: 0.9911 - val_auc: 0.7306\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0467 - acc: 0.9912 - auc: 0.7335 - val_loss: 0.0475 - val_acc: 0.9911 - val_auc: 0.7364\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7385 - val_loss: 0.0473 - val_acc: 0.9911 - val_auc: 0.7406\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0464 - acc: 0.9912 - auc: 0.7422 - val_loss: 0.0472 - val_acc: 0.9911 - val_auc: 0.7437\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7448 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7460\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 13s 20us/step - loss: 0.5708 - acc: 0.9064 - auc: 0.6339 - val_loss: 0.2874 - val_acc: 0.9899 - val_auc: 0.6443\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0950 - acc: 0.9903 - auc: 0.6682 - val_loss: 0.0512 - val_acc: 0.9904 - val_auc: 0.6925\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0484 - acc: 0.9906 - auc: 0.7074 - val_loss: 0.0481 - val_acc: 0.9909 - val_auc: 0.7186\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0472 - acc: 0.9909 - auc: 0.7258 - val_loss: 0.0475 - val_acc: 0.9910 - val_auc: 0.7314\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0467 - acc: 0.9910 - auc: 0.7350 - val_loss: 0.0472 - val_acc: 0.9911 - val_auc: 0.7385\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0464 - acc: 0.9911 - auc: 0.7408 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7429\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0462 - acc: 0.9911 - auc: 0.7446 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7458\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0460 - acc: 0.9911 - auc: 0.7470 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7480\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0459 - acc: 0.9911 - auc: 0.7488 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7497\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0458 - acc: 0.9912 - auc: 0.7504 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.2899 - acc: 0.9824 - auc: 0.5345 - val_loss: 0.1092 - val_acc: 0.9896 - val_auc: 0.6358\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0716 - acc: 0.9898 - auc: 0.6715 - val_loss: 0.0558 - val_acc: 0.9902 - val_auc: 0.6999\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0514 - acc: 0.9903 - auc: 0.7144 - val_loss: 0.0496 - val_acc: 0.9905 - val_auc: 0.7257\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0481 - acc: 0.9906 - auc: 0.7317 - val_loss: 0.0482 - val_acc: 0.9906 - val_auc: 0.7374\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0471 - acc: 0.9910 - auc: 0.7411 - val_loss: 0.0477 - val_acc: 0.9909 - val_auc: 0.7437\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0467 - acc: 0.9911 - auc: 0.7458 - val_loss: 0.0474 - val_acc: 0.9910 - val_auc: 0.7476\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7491 - val_loss: 0.0472 - val_acc: 0.9910 - val_auc: 0.7503\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9913 - auc: 0.7511 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7522\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0462 - acc: 0.9913 - auc: 0.7528 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7536\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0461 - acc: 0.9913 - auc: 0.7540 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7547\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9913 - auc: 0.7550 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7556\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0459 - acc: 0.9913 - auc: 0.7559 - val_loss: 0.0467 - val_acc: 0.9912 - val_auc: 0.7563\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7565 - val_loss: 0.0469 - val_acc: 0.9909 - val_auc: 0.7568\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7570 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7574\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7575 - val_loss: 0.0467 - val_acc: 0.9912 - val_auc: 0.7578\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7579 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7582\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.0456 - acc: 0.9914 - auc: 0.7585 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7586\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0455 - acc: 0.9914 - auc: 0.7587 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7589\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7590 - val_loss: 0.0464 - val_acc: 0.9912 - val_auc: 0.7592\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7593 - val_loss: 0.0464 - val_acc: 0.9912 - val_auc: 0.7594\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.3345 - acc: 0.9894 - auc: 0.5473 - val_loss: 0.1143 - val_acc: 0.9906 - val_auc: 0.6621\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0688 - acc: 0.9908 - auc: 0.6940 - val_loss: 0.0521 - val_acc: 0.9909 - val_auc: 0.7168\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0480 - acc: 0.9910 - auc: 0.7272 - val_loss: 0.0476 - val_acc: 0.9911 - val_auc: 0.7348\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0464 - acc: 0.9913 - auc: 0.7389 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7430\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9913 - auc: 0.7455 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7476\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7491 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7506\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9914 - auc: 0.7514 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7526\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0456 - acc: 0.9914 - auc: 0.7533 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7541\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9914 - auc: 0.7548 - val_loss: 0.0466 - val_acc: 0.9912 - val_auc: 0.7554\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7557 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7564\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7569 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7573\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7576 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7580\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7581 - val_loss: 0.0464 - val_acc: 0.9912 - val_auc: 0.7586\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7589 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7592\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7594 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7597\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7598 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7601\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7602 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7605\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7606 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7609\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7610 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7612\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7613 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7614\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.6746 - acc: 0.8972 - auc: 0.5896 - val_loss: 0.5957 - val_acc: 0.9891 - val_auc: 0.6201\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.3622 - acc: 0.9898 - auc: 0.6221 - val_loss: 0.1564 - val_acc: 0.9901 - val_auc: 0.6463\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 13s 19us/step - loss: 0.0908 - acc: 0.9902 - auc: 0.6648 - val_loss: 0.0614 - val_acc: 0.9903 - val_auc: 0.6818\n",
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0534 - acc: 0.9904 - auc: 0.6943 - val_loss: 0.0508 - val_acc: 0.9904 - val_auc: 0.7044\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0485 - acc: 0.9906 - auc: 0.7114 - val_loss: 0.0488 - val_acc: 0.9905 - val_auc: 0.7179\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0474 - acc: 0.9907 - auc: 0.7226 - val_loss: 0.0480 - val_acc: 0.9907 - val_auc: 0.7266\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0469 - acc: 0.9909 - auc: 0.7298 - val_loss: 0.0476 - val_acc: 0.9908 - val_auc: 0.7329\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0466 - acc: 0.9910 - auc: 0.7352 - val_loss: 0.0473 - val_acc: 0.9907 - val_auc: 0.7376\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0464 - acc: 0.9911 - auc: 0.7392 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7410\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0462 - acc: 0.9912 - auc: 0.7423 - val_loss: 0.0470 - val_acc: 0.9910 - val_auc: 0.7438\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0461 - acc: 0.9912 - auc: 0.7447 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7459\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7467 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7476\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0458 - acc: 0.9912 - auc: 0.7483 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7491\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0457 - acc: 0.9912 - auc: 0.7498 - val_loss: 0.0466 - val_acc: 0.9910 - val_auc: 0.7504\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0456 - acc: 0.9912 - auc: 0.7510 - val_loss: 0.0465 - val_acc: 0.9910 - val_auc: 0.7515\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7519 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7525\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7529 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7534\n",
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7538 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7542\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7544 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7549\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7551 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7555\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7557 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7560\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7562 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7565\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7567 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7569\n",
      "Epoch 24/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7571 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7573\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7575 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7577\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7579 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7581\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7582 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7584\n",
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7585 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7587\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7587 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7589\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7591 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7592\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 13s 20us/step - loss: 0.5074 - acc: 0.8960 - auc: 0.6597 - val_loss: 0.2500 - val_acc: 0.9903 - val_auc: 0.6539\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.1125 - acc: 0.9902 - auc: 0.6674 - val_loss: 0.0596 - val_acc: 0.9905 - val_auc: 0.6893\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0514 - acc: 0.9905 - auc: 0.7049 - val_loss: 0.0486 - val_acc: 0.9906 - val_auc: 0.7173\n",
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0468 - acc: 0.9910 - auc: 0.7254 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7324\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0461 - acc: 0.9912 - auc: 0.7372 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7407\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7431 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7458\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7475 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7492\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7505 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7517\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7526 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7536\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7543 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7551\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7557 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7564\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7568 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7574\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7578 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7583\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7586 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7591\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7594 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7597\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7600 - val_loss: 0.0459 - val_acc: 0.9912 - val_auc: 0.7603\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7605 - val_loss: 0.0459 - val_acc: 0.9910 - val_auc: 0.7608\n",
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7610 - val_loss: 0.0459 - val_acc: 0.9912 - val_auc: 0.7613\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7614 - val_loss: 0.0459 - val_acc: 0.9910 - val_auc: 0.7616\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7618 - val_loss: 0.0459 - val_acc: 0.9911 - val_auc: 0.7620\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7621 - val_loss: 0.0458 - val_acc: 0.9911 - val_auc: 0.7623\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7623 - val_loss: 0.0459 - val_acc: 0.9911 - val_auc: 0.7626\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7627 - val_loss: 0.0458 - val_acc: 0.9911 - val_auc: 0.7628\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7628 - val_loss: 0.0458 - val_acc: 0.9912 - val_auc: 0.7631\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0448 - acc: 0.9914 - auc: 0.7632 - val_loss: 0.0457 - val_acc: 0.9912 - val_auc: 0.7633\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0447 - acc: 0.9914 - auc: 0.7634 - val_loss: 0.0457 - val_acc: 0.9912 - val_auc: 0.7636\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0447 - acc: 0.9914 - auc: 0.7636 - val_loss: 0.0458 - val_acc: 0.9911 - val_auc: 0.7637\n",
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0447 - acc: 0.9914 - auc: 0.7638 - val_loss: 0.0457 - val_acc: 0.9911 - val_auc: 0.7639\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0447 - acc: 0.9914 - auc: 0.7640 - val_loss: 0.0458 - val_acc: 0.9912 - val_auc: 0.7641\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0447 - acc: 0.9915 - auc: 0.7642 - val_loss: 0.0457 - val_acc: 0.9911 - val_auc: 0.7642\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.4547 - acc: 0.9491 - auc: 0.7064 - val_loss: 0.2240 - val_acc: 0.9895 - val_auc: 0.6719\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.1269 - acc: 0.9898 - auc: 0.6704 - val_loss: 0.0761 - val_acc: 0.9903 - val_auc: 0.6833\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0612 - acc: 0.9903 - auc: 0.6940 - val_loss: 0.0530 - val_acc: 0.9905 - val_auc: 0.7048\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0509 - acc: 0.9907 - auc: 0.7131 - val_loss: 0.0504 - val_acc: 0.9908 - val_auc: 0.7205\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0494 - acc: 0.9908 - auc: 0.7256 - val_loss: 0.0497 - val_acc: 0.9908 - val_auc: 0.7300\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0487 - acc: 0.9908 - auc: 0.7330 - val_loss: 0.0492 - val_acc: 0.9908 - val_auc: 0.7361\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0483 - acc: 0.9909 - auc: 0.7382 - val_loss: 0.0489 - val_acc: 0.9908 - val_auc: 0.7403\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0479 - acc: 0.9909 - auc: 0.7417 - val_loss: 0.0486 - val_acc: 0.9908 - val_auc: 0.7432\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0477 - acc: 0.9909 - auc: 0.7443 - val_loss: 0.0484 - val_acc: 0.9908 - val_auc: 0.7455\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0475 - acc: 0.9910 - auc: 0.7462 - val_loss: 0.0482 - val_acc: 0.9910 - val_auc: 0.7472\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.4212 - acc: 0.9101 - auc: 0.5800 - val_loss: 0.1322 - val_acc: 0.9899 - val_auc: 0.6173\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0757 - acc: 0.9900 - auc: 0.6591 - val_loss: 0.0550 - val_acc: 0.9903 - val_auc: 0.6902\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0504 - acc: 0.9905 - auc: 0.7065 - val_loss: 0.0491 - val_acc: 0.9907 - val_auc: 0.7195\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0479 - acc: 0.9908 - auc: 0.7265 - val_loss: 0.0479 - val_acc: 0.9909 - val_auc: 0.7327\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0471 - acc: 0.9911 - auc: 0.7364 - val_loss: 0.0475 - val_acc: 0.9911 - val_auc: 0.7400\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0468 - acc: 0.9912 - auc: 0.7426 - val_loss: 0.0473 - val_acc: 0.9911 - val_auc: 0.7447\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7465 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7479\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0462 - acc: 0.9912 - auc: 0.7489 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7502\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7512 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7521\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7528 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7536\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.3549 - acc: 0.9874 - auc: 0.4877 - val_loss: 0.1703 - val_acc: 0.9897 - val_auc: 0.5908\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 13s 19us/step - loss: 0.1011 - acc: 0.9900 - auc: 0.6407 - val_loss: 0.0667 - val_acc: 0.9904 - val_auc: 0.6760\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0567 - acc: 0.9904 - auc: 0.6952 - val_loss: 0.0520 - val_acc: 0.9905 - val_auc: 0.7102\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0490 - acc: 0.9908 - auc: 0.7188 - val_loss: 0.0481 - val_acc: 0.9909 - val_auc: 0.7262\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0472 - acc: 0.9910 - auc: 0.7309 - val_loss: 0.0476 - val_acc: 0.9910 - val_auc: 0.7353\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0468 - acc: 0.9911 - auc: 0.7382 - val_loss: 0.0473 - val_acc: 0.9911 - val_auc: 0.7410\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7429 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7448\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7463 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7476\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0461 - acc: 0.9912 - auc: 0.7486 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7498\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7505 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7515\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7522 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7529\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9912 - auc: 0.7533 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7540\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7544 - val_loss: 0.0464 - val_acc: 0.9912 - val_auc: 0.7551\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7554 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7559\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7562 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7567\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7570 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7574\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7577 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7580\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7582 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7585\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7586 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7589\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7591 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.2034 - acc: 0.9874 - auc: 0.3797 - val_loss: 0.0547 - val_acc: 0.9902 - val_auc: 0.6030\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0492 - acc: 0.9905 - auc: 0.6676 - val_loss: 0.0481 - val_acc: 0.9909 - val_auc: 0.7022\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0472 - acc: 0.9910 - auc: 0.7168 - val_loss: 0.0477 - val_acc: 0.9909 - val_auc: 0.7273\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0468 - acc: 0.9912 - auc: 0.7326 - val_loss: 0.0474 - val_acc: 0.9911 - val_auc: 0.7379\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7410 - val_loss: 0.0472 - val_acc: 0.9911 - val_auc: 0.7436\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7453 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7472\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9912 - auc: 0.7484 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7497\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7506 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7517\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7523 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7532\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7536 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7545\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7549 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7555\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7560 - val_loss: 0.0464 - val_acc: 0.9912 - val_auc: 0.7564\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7567 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7572\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7575 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7578\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7580 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7584\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7586 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7589\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7591 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7594\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7596 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7598\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7599 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7601\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7602 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7604\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.3942 - acc: 0.9823 - auc: 0.3903 - val_loss: 0.1801 - val_acc: 0.9874 - val_auc: 0.4265\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0971 - acc: 0.9884 - auc: 0.4832 - val_loss: 0.0616 - val_acc: 0.9898 - val_auc: 0.5592\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0539 - acc: 0.9900 - auc: 0.6053 - val_loss: 0.0509 - val_acc: 0.9903 - val_auc: 0.6395\n",
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0489 - acc: 0.9904 - auc: 0.6602 - val_loss: 0.0489 - val_acc: 0.9904 - val_auc: 0.6771\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0477 - acc: 0.9905 - auc: 0.6882 - val_loss: 0.0484 - val_acc: 0.9908 - val_auc: 0.6979\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0473 - acc: 0.9908 - auc: 0.7048 - val_loss: 0.0481 - val_acc: 0.9908 - val_auc: 0.7108\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0470 - acc: 0.9910 - auc: 0.7152 - val_loss: 0.0479 - val_acc: 0.9909 - val_auc: 0.7194\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0468 - acc: 0.9911 - auc: 0.7225 - val_loss: 0.0477 - val_acc: 0.9911 - val_auc: 0.7256\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0466 - acc: 0.9912 - auc: 0.7280 - val_loss: 0.0476 - val_acc: 0.9910 - val_auc: 0.7301\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7320 - val_loss: 0.0474 - val_acc: 0.9911 - val_auc: 0.7337\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7351 - val_loss: 0.0473 - val_acc: 0.9911 - val_auc: 0.7366\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0462 - acc: 0.9913 - auc: 0.7376 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7389\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0461 - acc: 0.9913 - auc: 0.7399 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7409\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0460 - acc: 0.9913 - auc: 0.7418 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7426\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0459 - acc: 0.9913 - auc: 0.7434 - val_loss: 0.0468 - val_acc: 0.9912 - val_auc: 0.7441\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7448 - val_loss: 0.0467 - val_acc: 0.9912 - val_auc: 0.7454\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7460 - val_loss: 0.0466 - val_acc: 0.9912 - val_auc: 0.7466\n",
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7471 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7477\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7481 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7486\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7490 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7495\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7498 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7502\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7506 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7510\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7512 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7516\n",
      "Epoch 24/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7518 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7522\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0452 - acc: 0.9913 - auc: 0.7525 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7528\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0452 - acc: 0.9913 - auc: 0.7530 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7533\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7536 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7540 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7543\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7545 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7547\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7549 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7551\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.2904 - acc: 0.9315 - auc: 0.4325 - val_loss: 0.0614 - val_acc: 0.9903 - val_auc: 0.6303\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0515 - acc: 0.9905 - auc: 0.6835 - val_loss: 0.0494 - val_acc: 0.9907 - val_auc: 0.7098\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0479 - acc: 0.9910 - auc: 0.7218 - val_loss: 0.0482 - val_acc: 0.9909 - val_auc: 0.7305\n",
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0472 - acc: 0.9911 - auc: 0.7353 - val_loss: 0.0478 - val_acc: 0.9910 - val_auc: 0.7394\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0469 - acc: 0.9912 - auc: 0.7420 - val_loss: 0.0475 - val_acc: 0.9911 - val_auc: 0.7443\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0466 - acc: 0.9912 - auc: 0.7458 - val_loss: 0.0473 - val_acc: 0.9911 - val_auc: 0.7474\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0464 - acc: 0.9912 - auc: 0.7484 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7495\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0462 - acc: 0.9912 - auc: 0.7501 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7510\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0461 - acc: 0.9912 - auc: 0.7516 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7521\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9912 - auc: 0.7525 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7530\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7532 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7537\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0458 - acc: 0.9912 - auc: 0.7540 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7543\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0457 - acc: 0.9912 - auc: 0.7546 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7549\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7552 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7554\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7557 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7559\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7561 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7563\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7565 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7568\n",
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9913 - auc: 0.7570 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7572\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7573 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7576\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7577 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7579\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9913 - auc: 0.7581 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7582\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7584 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7586\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7586 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7589\n",
      "Epoch 24/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7590 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7591\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9913 - auc: 0.7592 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7594\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7595 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7596\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0451 - acc: 0.9913 - auc: 0.7597 - val_loss: 0.0461 - val_acc: 0.9911 - val_auc: 0.7599\n",
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 13s 20us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7600 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7601\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7602 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7603\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7604 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7605\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.5445 - acc: 0.9851 - auc: 0.7255 - val_loss: 0.3640 - val_acc: 0.9881 - val_auc: 0.6846\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.2282 - acc: 0.9890 - auc: 0.6689 - val_loss: 0.1343 - val_acc: 0.9904 - val_auc: 0.6810\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0873 - acc: 0.9907 - auc: 0.6974 - val_loss: 0.0611 - val_acc: 0.9909 - val_auc: 0.7112\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0531 - acc: 0.9912 - auc: 0.7200 - val_loss: 0.0501 - val_acc: 0.9911 - val_auc: 0.7268\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0480 - acc: 0.9912 - auc: 0.7311 - val_loss: 0.0480 - val_acc: 0.9911 - val_auc: 0.7353\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0469 - acc: 0.9912 - auc: 0.7381 - val_loss: 0.0474 - val_acc: 0.9911 - val_auc: 0.7408\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0465 - acc: 0.9913 - auc: 0.7429 - val_loss: 0.0471 - val_acc: 0.9911 - val_auc: 0.7447\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7464 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7476\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0461 - acc: 0.9912 - auc: 0.7487 - val_loss: 0.0468 - val_acc: 0.9912 - val_auc: 0.7497\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0459 - acc: 0.9913 - auc: 0.7506 - val_loss: 0.0467 - val_acc: 0.9912 - val_auc: 0.7515\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/10\n",
      "666805/666805 [==============================] - 14s 20us/step - loss: 0.1510 - acc: 0.9855 - auc: 0.4074 - val_loss: 0.0605 - val_acc: 0.9858 - val_auc: 0.5547\n",
      "Epoch 2/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0552 - acc: 0.9855 - auc: 0.6267 - val_loss: 0.0528 - val_acc: 0.9858 - val_auc: 0.6714\n",
      "Epoch 3/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0517 - acc: 0.9855 - auc: 0.6923 - val_loss: 0.0515 - val_acc: 0.9858 - val_auc: 0.7077\n",
      "Epoch 4/10\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.0508 - acc: 0.9855 - auc: 0.7165 - val_loss: 0.0510 - val_acc: 0.9858 - val_auc: 0.7238\n",
      "Epoch 5/10\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.0503 - acc: 0.9855 - auc: 0.7287 - val_loss: 0.0506 - val_acc: 0.9858 - val_auc: 0.7326\n",
      "Epoch 6/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0498 - acc: 0.9855 - auc: 0.7355 - val_loss: 0.0502 - val_acc: 0.9858 - val_auc: 0.7383\n",
      "Epoch 7/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0495 - acc: 0.9855 - auc: 0.7401 - val_loss: 0.0499 - val_acc: 0.9858 - val_auc: 0.7422\n",
      "Epoch 8/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0492 - acc: 0.9860 - auc: 0.7436 - val_loss: 0.0498 - val_acc: 0.9905 - val_auc: 0.7450\n",
      "Epoch 9/10\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0491 - acc: 0.9909 - auc: 0.7462 - val_loss: 0.0497 - val_acc: 0.9906 - val_auc: 0.7472\n",
      "Epoch 10/10\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0489 - acc: 0.9910 - auc: 0.7481 - val_loss: 0.0495 - val_acc: 0.9906 - val_auc: 0.7490\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 13s 19us/step - loss: 0.3634 - acc: 0.9889 - auc: 0.5127 - val_loss: 0.1749 - val_acc: 0.9903 - val_auc: 0.6106\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0988 - acc: 0.9903 - auc: 0.6523 - val_loss: 0.0608 - val_acc: 0.9904 - val_auc: 0.6839\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0512 - acc: 0.9906 - auc: 0.7006 - val_loss: 0.0484 - val_acc: 0.9907 - val_auc: 0.7143\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0472 - acc: 0.9908 - auc: 0.7226 - val_loss: 0.0476 - val_acc: 0.9908 - val_auc: 0.7296\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0468 - acc: 0.9910 - auc: 0.7338 - val_loss: 0.0474 - val_acc: 0.9909 - val_auc: 0.7379\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0466 - acc: 0.9911 - auc: 0.7404 - val_loss: 0.0472 - val_acc: 0.9910 - val_auc: 0.7430\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0464 - acc: 0.9911 - auc: 0.7445 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7465\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7479 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7490\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7498 - val_loss: 0.0470 - val_acc: 0.9911 - val_auc: 0.7508\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0462 - acc: 0.9912 - auc: 0.7516 - val_loss: 0.0469 - val_acc: 0.9910 - val_auc: 0.7523\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0461 - acc: 0.9912 - auc: 0.7530 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7535\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0460 - acc: 0.9912 - auc: 0.7540 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7546\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9912 - auc: 0.7549 - val_loss: 0.0468 - val_acc: 0.9910 - val_auc: 0.7554\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0459 - acc: 0.9913 - auc: 0.7557 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7562\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 10s 16us/step - loss: 0.0459 - acc: 0.9912 - auc: 0.7565 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7568\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0458 - acc: 0.9912 - auc: 0.7571 - val_loss: 0.0467 - val_acc: 0.9910 - val_auc: 0.7574\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0458 - acc: 0.9912 - auc: 0.7576 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7580\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9912 - auc: 0.7582 - val_loss: 0.0466 - val_acc: 0.9911 - val_auc: 0.7584\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0457 - acc: 0.9912 - auc: 0.7586 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7589\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0457 - acc: 0.9912 - auc: 0.7591 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7592\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/20\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.5675 - acc: 0.7641 - auc: 0.6766 - val_loss: 0.3943 - val_acc: 0.9900 - val_auc: 0.6573\n",
      "Epoch 2/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.2991 - acc: 0.9903 - auc: 0.6632 - val_loss: 0.2203 - val_acc: 0.9905 - val_auc: 0.6736\n",
      "Epoch 3/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.1698 - acc: 0.9906 - auc: 0.6836 - val_loss: 0.1313 - val_acc: 0.9906 - val_auc: 0.6927\n",
      "Epoch 4/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0782 - acc: 0.9909 - auc: 0.6999 - val_loss: 0.0488 - val_acc: 0.9907 - val_auc: 0.7087\n",
      "Epoch 5/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0471 - acc: 0.9909 - auc: 0.7150 - val_loss: 0.0477 - val_acc: 0.9908 - val_auc: 0.7208\n",
      "Epoch 6/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0467 - acc: 0.9909 - auc: 0.7247 - val_loss: 0.0475 - val_acc: 0.9909 - val_auc: 0.7288\n",
      "Epoch 7/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9909 - auc: 0.7316 - val_loss: 0.0474 - val_acc: 0.9907 - val_auc: 0.7343\n",
      "Epoch 8/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9910 - auc: 0.7362 - val_loss: 0.0472 - val_acc: 0.9910 - val_auc: 0.7384\n",
      "Epoch 9/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0462 - acc: 0.9911 - auc: 0.7402 - val_loss: 0.0470 - val_acc: 0.9909 - val_auc: 0.7416\n",
      "Epoch 10/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9911 - auc: 0.7429 - val_loss: 0.0468 - val_acc: 0.9909 - val_auc: 0.7441\n",
      "Epoch 11/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0459 - acc: 0.9911 - auc: 0.7451 - val_loss: 0.0467 - val_acc: 0.9910 - val_auc: 0.7462\n",
      "Epoch 12/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9912 - auc: 0.7471 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7480\n",
      "Epoch 13/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9912 - auc: 0.7486 - val_loss: 0.0464 - val_acc: 0.9910 - val_auc: 0.7495\n",
      "Epoch 14/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9912 - auc: 0.7501 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7508\n",
      "Epoch 15/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9912 - auc: 0.7512 - val_loss: 0.0464 - val_acc: 0.9911 - val_auc: 0.7519\n",
      "Epoch 16/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0454 - acc: 0.9912 - auc: 0.7523 - val_loss: 0.0463 - val_acc: 0.9910 - val_auc: 0.7529\n",
      "Epoch 17/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7533 - val_loss: 0.0463 - val_acc: 0.9911 - val_auc: 0.7538\n",
      "Epoch 18/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7543 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7546\n",
      "Epoch 19/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9913 - auc: 0.7549 - val_loss: 0.0462 - val_acc: 0.9911 - val_auc: 0.7553\n",
      "Epoch 20/20\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0452 - acc: 0.9913 - auc: 0.7556 - val_loss: 0.0462 - val_acc: 0.9910 - val_auc: 0.7560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.3451 - acc: 0.9593 - auc: 0.6175 - val_loss: 0.1352 - val_acc: 0.9872 - val_auc: 0.6565\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0860 - acc: 0.9870 - auc: 0.6705 - val_loss: 0.0642 - val_acc: 0.9878 - val_auc: 0.6881\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0580 - acc: 0.9881 - auc: 0.6995 - val_loss: 0.0550 - val_acc: 0.9882 - val_auc: 0.7093\n",
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0532 - acc: 0.9881 - auc: 0.7164 - val_loss: 0.0525 - val_acc: 0.9882 - val_auc: 0.7220\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0515 - acc: 0.9883 - auc: 0.7262 - val_loss: 0.0513 - val_acc: 0.9885 - val_auc: 0.7303\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0505 - acc: 0.9884 - auc: 0.7332 - val_loss: 0.0506 - val_acc: 0.9885 - val_auc: 0.7359\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0499 - acc: 0.9884 - auc: 0.7381 - val_loss: 0.0502 - val_acc: 0.9886 - val_auc: 0.7400\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0495 - acc: 0.9884 - auc: 0.7413 - val_loss: 0.0499 - val_acc: 0.9886 - val_auc: 0.7429\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0492 - acc: 0.9884 - auc: 0.7440 - val_loss: 0.0497 - val_acc: 0.9886 - val_auc: 0.7452\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0490 - acc: 0.9885 - auc: 0.7460 - val_loss: 0.0495 - val_acc: 0.9886 - val_auc: 0.7470\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0488 - acc: 0.9886 - auc: 0.7477 - val_loss: 0.0494 - val_acc: 0.9888 - val_auc: 0.7485\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0486 - acc: 0.9887 - auc: 0.7490 - val_loss: 0.0491 - val_acc: 0.9889 - val_auc: 0.7497\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0480 - acc: 0.9896 - auc: 0.7502 - val_loss: 0.0480 - val_acc: 0.9907 - val_auc: 0.7508\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0469 - acc: 0.9910 - auc: 0.7513 - val_loss: 0.0476 - val_acc: 0.9909 - val_auc: 0.7518\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0466 - acc: 0.9912 - auc: 0.7523 - val_loss: 0.0475 - val_acc: 0.9909 - val_auc: 0.7527\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7530 - val_loss: 0.0475 - val_acc: 0.9910 - val_auc: 0.7534\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7536 - val_loss: 0.0474 - val_acc: 0.9910 - val_auc: 0.7540\n",
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0462 - acc: 0.9912 - auc: 0.7543 - val_loss: 0.0473 - val_acc: 0.9910 - val_auc: 0.7546\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0461 - acc: 0.9913 - auc: 0.7548 - val_loss: 0.0472 - val_acc: 0.9910 - val_auc: 0.7550\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0461 - acc: 0.9913 - auc: 0.7552 - val_loss: 0.0472 - val_acc: 0.9910 - val_auc: 0.7555\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0460 - acc: 0.9913 - auc: 0.7556 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7559\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0459 - acc: 0.9913 - auc: 0.7560 - val_loss: 0.0471 - val_acc: 0.9910 - val_auc: 0.7562\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0459 - acc: 0.9913 - auc: 0.7564 - val_loss: 0.0470 - val_acc: 0.9910 - val_auc: 0.7566\n",
      "Epoch 24/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7567 - val_loss: 0.0470 - val_acc: 0.9910 - val_auc: 0.7569\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7570 - val_loss: 0.0469 - val_acc: 0.9911 - val_auc: 0.7572\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 9s 14us/step - loss: 0.0457 - acc: 0.9914 - auc: 0.7572 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7574\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0456 - acc: 0.9914 - auc: 0.7575 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7577\n",
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0456 - acc: 0.9914 - auc: 0.7577 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7579\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9914 - auc: 0.7580 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7581\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 10s 14us/step - loss: 0.0455 - acc: 0.9914 - auc: 0.7581 - val_loss: 0.0467 - val_acc: 0.9911 - val_auc: 0.7583\n",
      "Train on 666805 samples, validate on 166702 samples\n",
      "Epoch 1/30\n",
      "666805/666805 [==============================] - 12s 19us/step - loss: 0.5515 - acc: 0.8607 - auc: 0.7050 - val_loss: 0.2545 - val_acc: 0.9897 - val_auc: 0.6817\n",
      "Epoch 2/30\n",
      "666805/666805 [==============================] - 12s 17us/step - loss: 0.1196 - acc: 0.9903 - auc: 0.6878 - val_loss: 0.0637 - val_acc: 0.9907 - val_auc: 0.7053\n",
      "Epoch 3/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0528 - acc: 0.9909 - auc: 0.7169 - val_loss: 0.0478 - val_acc: 0.9910 - val_auc: 0.7274\n",
      "Epoch 4/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0470 - acc: 0.9911 - auc: 0.7327 - val_loss: 0.0476 - val_acc: 0.9911 - val_auc: 0.7372\n",
      "Epoch 5/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0467 - acc: 0.9912 - auc: 0.7401 - val_loss: 0.0474 - val_acc: 0.9911 - val_auc: 0.7430\n",
      "Epoch 6/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0465 - acc: 0.9912 - auc: 0.7448 - val_loss: 0.0472 - val_acc: 0.9911 - val_auc: 0.7467\n",
      "Epoch 7/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0463 - acc: 0.9912 - auc: 0.7479 - val_loss: 0.0470 - val_acc: 0.9912 - val_auc: 0.7494\n",
      "Epoch 8/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0462 - acc: 0.9912 - auc: 0.7505 - val_loss: 0.0468 - val_acc: 0.9911 - val_auc: 0.7515\n",
      "Epoch 9/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0460 - acc: 0.9912 - auc: 0.7521 - val_loss: 0.0467 - val_acc: 0.9912 - val_auc: 0.7531\n",
      "Epoch 10/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0459 - acc: 0.9913 - auc: 0.7536 - val_loss: 0.0466 - val_acc: 0.9912 - val_auc: 0.7543\n",
      "Epoch 11/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0458 - acc: 0.9913 - auc: 0.7548 - val_loss: 0.0465 - val_acc: 0.9912 - val_auc: 0.7554\n",
      "Epoch 12/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0457 - acc: 0.9913 - auc: 0.7557 - val_loss: 0.0465 - val_acc: 0.9911 - val_auc: 0.7563\n",
      "Epoch 13/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0456 - acc: 0.9913 - auc: 0.7565 - val_loss: 0.0464 - val_acc: 0.9912 - val_auc: 0.7571\n",
      "Epoch 14/30\n",
      "666805/666805 [==============================] - 10s 15us/step - loss: 0.0455 - acc: 0.9913 - auc: 0.7573 - val_loss: 0.0463 - val_acc: 0.9912 - val_auc: 0.7577\n",
      "Epoch 15/30\n",
      "666805/666805 [==============================] - 12s 18us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7580 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7583\n",
      "Epoch 16/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0454 - acc: 0.9914 - auc: 0.7585 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7588\n",
      "Epoch 17/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0453 - acc: 0.9913 - auc: 0.7591 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0453 - acc: 0.9914 - auc: 0.7595 - val_loss: 0.0462 - val_acc: 0.9912 - val_auc: 0.7598\n",
      "Epoch 19/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7599 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7601\n",
      "Epoch 20/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0452 - acc: 0.9914 - auc: 0.7603 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7605\n",
      "Epoch 21/30\n",
      "666805/666805 [==============================] - 11s 17us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7606 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7608\n",
      "Epoch 22/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7610 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7611\n",
      "Epoch 23/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7612 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7614\n",
      "Epoch 24/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0451 - acc: 0.9914 - auc: 0.7615 - val_loss: 0.0461 - val_acc: 0.9912 - val_auc: 0.7617\n",
      "Epoch 25/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7617 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7619\n",
      "Epoch 26/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7619 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7621\n",
      "Epoch 27/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0450 - acc: 0.9914 - auc: 0.7622 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7623\n",
      "Epoch 28/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7624 - val_loss: 0.0460 - val_acc: 0.9912 - val_auc: 0.7625\n",
      "Epoch 29/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7626 - val_loss: 0.0459 - val_acc: 0.9912 - val_auc: 0.7627\n",
      "Epoch 30/30\n",
      "666805/666805 [==============================] - 11s 16us/step - loss: 0.0449 - acc: 0.9914 - auc: 0.7628 - val_loss: 0.0460 - val_acc: 0.9911 - val_auc: 0.7629\n"
     ]
    }
   ],
   "source": [
    "for first_layer in [16,10]:\n",
    "    for second_layer in [12,7]:\n",
    "        for epoch in [10,20,30]:\n",
    "            for batchsize in [6400,3200]:\n",
    "                keras.backend.clear_session()\n",
    "                m = train_model(first_layer, second_layer, epoch, batchsize)\n",
    "                fdr_train = FDR(x_train,y_train,m)\n",
    "                fdr_test = FDR(x_test,y_test,m)\n",
    "                fdr_oot = FDR(x_oot,y_oot,m)\n",
    "                result = result.append({'first_layer':first_layer,'second_layer':second_layer, 'epoch':epoch,'batchsize':batchsize,'train_fdr':fdr_train,'test_fdr':fdr_test,'oot_fdr':fdr_oot}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_layer</th>\n",
       "      <th>second_layer</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>train_fdr</th>\n",
       "      <th>test_fdr</th>\n",
       "      <th>oot_fdr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.537544</td>\n",
       "      <td>0.516279</td>\n",
       "      <td>0.515088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.534744</td>\n",
       "      <td>0.516279</td>\n",
       "      <td>0.515926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.538996</td>\n",
       "      <td>0.517548</td>\n",
       "      <td>0.517603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.538063</td>\n",
       "      <td>0.519239</td>\n",
       "      <td>0.519698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.538270</td>\n",
       "      <td>0.517970</td>\n",
       "      <td>0.515507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.538477</td>\n",
       "      <td>0.518393</td>\n",
       "      <td>0.522213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.529247</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.529973</td>\n",
       "      <td>0.509937</td>\n",
       "      <td>0.503772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.533603</td>\n",
       "      <td>0.512896</td>\n",
       "      <td>0.511316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.536611</td>\n",
       "      <td>0.515011</td>\n",
       "      <td>0.515507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.517970</td>\n",
       "      <td>0.522632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.538789</td>\n",
       "      <td>0.519662</td>\n",
       "      <td>0.517184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.521883</td>\n",
       "      <td>0.503594</td>\n",
       "      <td>0.494971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.532670</td>\n",
       "      <td>0.515433</td>\n",
       "      <td>0.512573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.537648</td>\n",
       "      <td>0.518816</td>\n",
       "      <td>0.523051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.537337</td>\n",
       "      <td>0.518393</td>\n",
       "      <td>0.516345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.537959</td>\n",
       "      <td>0.516702</td>\n",
       "      <td>0.515926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.537544</td>\n",
       "      <td>0.518816</td>\n",
       "      <td>0.522632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.531944</td>\n",
       "      <td>0.510359</td>\n",
       "      <td>0.511316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.530803</td>\n",
       "      <td>0.510359</td>\n",
       "      <td>0.510059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.534951</td>\n",
       "      <td>0.515856</td>\n",
       "      <td>0.516764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.538270</td>\n",
       "      <td>0.517548</td>\n",
       "      <td>0.522632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>0.532047</td>\n",
       "      <td>0.509937</td>\n",
       "      <td>0.508382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.538892</td>\n",
       "      <td>0.517970</td>\n",
       "      <td>0.523051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    first_layer  second_layer  epoch  batchsize  train_fdr  test_fdr   oot_fdr\n",
       "0          16.0          12.0   10.0     6400.0   0.537544  0.516279  0.515088\n",
       "1          16.0          12.0   10.0     3200.0   0.534744  0.516279  0.515926\n",
       "2          16.0          12.0   20.0     6400.0   0.538996  0.517548  0.517603\n",
       "3          16.0          12.0   20.0     3200.0   0.538063  0.519239  0.519698\n",
       "4          16.0          12.0   30.0     6400.0   0.538270  0.517970  0.515507\n",
       "5          16.0          12.0   30.0     3200.0   0.538477  0.518393  0.522213\n",
       "6          16.0           7.0   10.0     6400.0   0.529247  0.509091  0.500000\n",
       "7          16.0           7.0   10.0     3200.0   0.529973  0.509937  0.503772\n",
       "8          16.0           7.0   20.0     6400.0   0.533603  0.512896  0.511316\n",
       "9          16.0           7.0   20.0     3200.0   0.536611  0.515011  0.515507\n",
       "10         16.0           7.0   30.0     6400.0   0.536300  0.517970  0.522632\n",
       "11         16.0           7.0   30.0     3200.0   0.538789  0.519662  0.517184\n",
       "12         10.0          12.0   10.0     6400.0   0.521883  0.503594  0.494971\n",
       "13         10.0          12.0   10.0     3200.0   0.532670  0.515433  0.512573\n",
       "14         10.0          12.0   20.0     6400.0   0.537648  0.518816  0.523051\n",
       "15         10.0          12.0   20.0     3200.0   0.537337  0.518393  0.516345\n",
       "16         10.0          12.0   30.0     6400.0   0.537959  0.516702  0.515926\n",
       "17         10.0          12.0   30.0     3200.0   0.537544  0.518816  0.522632\n",
       "18         10.0           7.0   10.0     6400.0   0.531944  0.510359  0.511316\n",
       "19         10.0           7.0   10.0     3200.0   0.530803  0.510359  0.510059\n",
       "20         10.0           7.0   20.0     6400.0   0.534951  0.515856  0.516764\n",
       "21         10.0           7.0   20.0     3200.0   0.538270  0.517548  0.522632\n",
       "22         10.0           7.0   30.0     6400.0   0.532047  0.509937  0.508382\n",
       "23         10.0           7.0   30.0     3200.0   0.538892  0.517970  0.523051"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_neural_net.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
